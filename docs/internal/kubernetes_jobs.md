# Scaling Data Validation with Kubernetes Jobs 

## Nature of Data Validation
Data Validation by nature is a batch process. We are presented with a set of arguments, the validation is performed, the results are provided and Data Validation completes. Data Validation can also take time (multiple secs, minutes) if a large amount of data needs to be validated. 

Data Validation has the `generate-table-partitions` function that partitions a row validation into a specified number of smaller, equally sized validations. Using this feature, validation of two large tables can be split into a number validations of partitions of the same tables. See [partition table PRD](partition_table_prd.md) for details on partitioning. This process generates a sequence of yaml files which can be used to validate the individual partitions. 

## Kubernetes Workloads
Kubernetes supports different types of workloads including a few batch workload types. The Job workload is a batch workload that retries execution until a specified number of them successfully complete. If a row validation has been split into `n` partitions, then we need to validate each partition and merge the results of the validations. Using Kubernetes Jobs we need to successfully run `n` completions of the job, as long as we guarantee that each completion is associated with a different partition. Kubernetes provides a type of job management called indexed completions that supports the Data Validation use case. A Kubernetes job can use multiple parallel worker processes. Each worker process has an index number that the control plane sets which identifies which part of of the overall task (i.e. which partition) to work on. The index is available in the environment variable `JOB_COMPLETION_INDEX` (in cloud run the environment variable is `CLOUD_RUN_TASK_INDEX`). An explanation of this is provided in [Introducing Indexed Jobs](https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/#:~:text=Indexed%20%3A%20the%20Job%20is%20considered,and%20the%20JOB_COMPLETION_INDEX%20environment%20variable).

Indexed completion mode supports partitioned yaml files generated by `generate-table-partitions` in Data Validation, if each worker process ran only the yaml file corresponding to its index. I have an introduced an optional parameter `--kube-completions` or `-kc`. When this flag is used with `data-validation configs run -cdir` with a config directory and the container runs in indexed jobs mode, each container only processes the specific validation yaml file corresponding to its index. If the flag is used with `data-validation configs run -cdir` with a config directory and DVT is not running in indexed jobs mode, a warning is issued. In other uses of `configs` such as `configs list` and `configs run -c`, this flag is ignored.
### IAM Permissions
This section is yet to be written and can be updated with the required IAM permissions. This section probably belongs in the external facing documentation.
### Passing database connection parameters
DVT database connection parameters are saved in `$HOME/.config/google-pso-data-validator` directory with passwords in raw text. With Kubernetes, DVT cannot depend on `.config` directory holding the connections unless they are baked into the image (or mounted as a volume - see Cloud Run limitation below) - which will require each customer to modify the container image we provide. A better approach (for regular and containerized DVT) would be use the (GCP) Secret Manager and retrieve connection credentials as a JSON object when we connect to the database. DVT currently uses the Secret Manager for retrieving secrets and stores them in the `.config` directory when the connections are added. This seems rather odd as it defeats the main purposes of using the secret manager - security and password rotation. 

I am proposing a simple command line change to resolve this issue. Whenever a connection parameter is specified, allow the user to optionally specify a secret manager (provider, project-id). If a secret manager is specified, then DVT retrieves the connection information directly from the secret manager at the time of creating the connection. This is the recommended approach to handle secrets as opposed to mounting secrets as volumes. Cloud Run also has a limitation that multiple secrets [cannot be mounted with the same path](https://cloud.google.com/run/docs/configuring/services/secrets#disallowed_paths_and_limitations). Since DVT requires connections to two different databases with the connection info being mounted in the same directory, i.e. `$HOME/.config/google-pso-data-validator`, DVT cannot run within Cloud Run without this change. With this change, DVT can be run in a container in Cloud Run or Kubernetes fetching the connection information from the GCP Secret Manager. 

This command line change also requires a change to the yaml file format. The secret manager parameters (provider, project-id), if provided will need to be stored in the yaml file so that DVT can retrieve it while making the connection. If we don't change the yaml file, Cloud Run could be used to run validations, but cannot run validations from a yaml file - defeating it usability to scale DVT. The section of the current yaml file looks like
```
result_handler:
  project_id: mudu-plex-pri
  table_id: pso_data_validator.results
  type: BigQuery
source: my_postgres
target: my_postgres
validations:
```
after this change this section of the yaml file will look like this:
```
result_handler:
  project_id: mudu-plex-pri
  table_id: pso_data_validator.results
  type: BigQuery
secret_manager_type: gcp
secret_manager_project_id: mudu-plex-pri
source: my_postgres
target: my_postgres
validations:
```
The change to the yaml file can be compatible, both backwards and forwards. When the new version of DVT reads a yaml file into a dict, it will start with a dict that looks like this: ```{secret_manager_type: None,
secret_manager_project_id: None}``` which will be updated by the dict from the yaml file. If an old yaml file is used, the values will be set correctly. When the old version of DVT reads a new yaml file with values for ```secret_manager_type```, ```secret_manager_project_id```, these values will be quietly ignored. If there are no connections listed in the old DVT (which is likely), the validation will fail saying: `No such file or directory: '/home/user/.config/google-pso-data-validator/xxxx.connection.json'`

Once this functionality is implemented, the existing functionality of retrieving connection information from the secret manger and storing them in `.config directory` becomes redundant. It can be removed to avoid confusion. 
## Future Work